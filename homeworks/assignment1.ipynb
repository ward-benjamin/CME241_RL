{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIwoQi_ZJOPD"
      },
      "source": [
        "# Stanford CME 241 (Winter 2026) - Assignment 1\n",
        "\n",
        "**Due: Friday, January 23 @ 11:59 PM PST on Gradescope.**\n",
        "\n",
        "Assignment instructions:\n",
        "- Make sure each of the subquestions have answers\n",
        "- Ensure that group members indicate which problems they're in charge of\n",
        "- Show work and walk through your thought process where applicable\n",
        "- Empty code blocks are for your use, so feel free to create more under each section as needed\n",
        "- Document code with light comments (i.e. 'this function handles visualization')\n",
        "\n",
        "Submission instructions:\n",
        "- When complete, fill out your publicly available GitHub repo file URL and group members below, then export or print this .ipynb file to PDF and upload the PDF to Gradescope.\n",
        "\n",
        "*Link to this ipynb file in your public GitHub repo (replace below URL with yours):*\n",
        "\n",
        "https://github.com/ward-benjamin/CME241_RL/blob/master/homeworks/assignment1.ipynb\n",
        "\n",
        "*Group members (replace below names with people in your group):*\n",
        "- Benjamin Ward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZupXsgsIJOPE"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path(\"..\").resolve()))\n",
        "sys.path.insert(0, str(Path(\"..\").resolve()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from typing import Dict, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rl.distribution import Categorical\n",
        "from rl.markov_process import FiniteMarkovProcess, NonTerminal, Terminal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcvIKfARJOPF"
      },
      "source": [
        "## Question 1: Snakes and Ladders (Led by Benjamin Ward)\n",
        "\n",
        "In the classic childhood game of Snakes and Ladders, all players start to the left of square 1 (call this position 0) and roll a 6-sided die to represent the number of squares they can move forward. The goal is to reach square 100 as quickly as possible. Landing on the bottom rung of a ladder allows for an automatic free-pass to climb, e.g. square 4 sends you directly to 14; whereas landing on a snake's head forces one to slide all the way to the tail, e.g. square 34 sends you to 6. Note, this game can be viewed as a Markov Process, where the outcome is only depedent on the current state and not the prior trajectory. In this question, we will ask you to both formally describe the Markov Process that describes this game, followed by coding up a version of the game to get familiar with the RL-book libraries.\n",
        "\n",
        "\n",
        "### Problem Statement\n",
        "\n",
        "How can we model this problem with a Markov Process?\n",
        "\n",
        "---\n",
        "\n",
        "### Subquestions\n",
        "\n",
        "#### Part (A): MDP Modeling\n",
        "\n",
        "Formalize the state space of the Snakes and Ladders game. Don't forget to specify the terminal state!\n",
        "\n",
        "---\n",
        "\n",
        "#### Part (B): Transition Probabilities\n",
        "\n",
        "Write out the structure of the transition probabilities. Feel free to abbreviate all squares that do not have a snake or ladder.\n",
        "\n",
        "---\n",
        "\n",
        "#### Part (C): Modeling the Game\n",
        "\n",
        "Code up a `transition_map: Transition[S]` data structure to represent the transition probabilities of the Snakes and Ladders Markov Process so you can model the game as an instance of `FiniteMarkovProcess`. Use the `traces` method to create sampling traces, and plot the graph of the distribution of time steps to finish the game. Use the image provided for the locations of the snakes and ladders.\n",
        "\n",
        "https://drive.google.com/file/d/1yhP242sG092Ico_WOPKrUp8jVJHbuGHH/view?usp=sharing\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2VHXTG2JOPG"
      },
      "source": [
        "### Part (A) Answer\n",
        "State space: S = {0,1,2,...,100} (we include 0 as the true \"start\" (before we're actually on the map))\n",
        "\n",
        "Terminal state: T = {100}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iQn1b0OJOPG"
      },
      "source": [
        "### Part (B) Answer\n",
        "There are 3 cases: either the state is the bottom rung of a ladder (case 1), either the state is the head of a snake (case 2), or the state is \"normal\" (case 3) (defined as not bottom of a ladder and not head of snake). Let's note x a given state. If x is bottom ladder, then top of ladder state is noted $l_x$, if x is head of snake, then tail of snake state is noted $s_x$.\n",
        "- Case 1: then $\\mathbb{P}(l_x\\lvert x) = 1$\n",
        "- Case 2: then $\\mathbb{P}(s_x\\lvert x) = 1$\n",
        "- Case 3: let's note $D_x=\\{\\min(100,x+k), 1\\leq k\\leq 6\\}$. Then $\\forall t\\in D_x \\text{ with } t<100, \\mathbb{P}[t\\lvert x] = \\frac{1}{6}$. If $100\\in D_x$, $\\mathbb{P}[100\\lvert x] = \\frac{7-\\lvert D_x\\lvert}{6}$\n",
        "\n",
        "Note that we made an assumption here that if we go above 100, we also finish the game. This enables us to consider 100 as the unique terminal state. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAC0uAmpJOPH"
      },
      "source": [
        "### Part (C) Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "98uhXDn6JOPH"
      },
      "outputs": [],
      "source": [
        "# fill in with Python code\n",
        "jump = {\n",
        "    1: 19, 4: 14, 8: 29, 21: 42, 28: 84, 51: 67, 71: 91, 80: 100,     # ladders\n",
        "    97: 78, 95: 56, 88: 24, 62: 18, 48: 26, 36: 6, 32: 10     # snakes\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_snakes_ladders_transition_map() -> dict[int, Categorical[int]]:\n",
        "    transition_map: dict[int, Categorical[int]] = {}\n",
        "    for s in range(0, 100):  # non terminal states\n",
        "        probs = defaultdict(float)\n",
        "        for k in range(1, 7):\n",
        "            raw_spot = min(s + k, 100)       # clip at 100 (we consider \"done\" if above 100)\n",
        "            # if we are at Case 1 (bottom ladder) or Case 2 (head snake) - it's a jump, else it's the right spot\n",
        "            nxt_spot = jump.get(raw_spot, raw_spot)   \n",
        "            probs[nxt_spot] += 1/6\n",
        "        transition_map[s] = Categorical(dict(probs))\n",
        "    return transition_map\n",
        "\n",
        "def steps_to_terminal_100(trace) -> int:\n",
        "    for t, st in enumerate(trace):\n",
        "        if isinstance(st, Terminal) and st.state == 100:\n",
        "            return t  # t steps from start\n",
        "    return t  # fallback "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "transition_map = build_snakes_ladders_transition_map()\n",
        "markov_process = FiniteMarkovProcess(transition_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We always start at state 0\n",
        "start_dist = Categorical({NonTerminal(0): 1.0})\n",
        "N_iter = 100000 # number of samples for average\n",
        "finish_steps = []\n",
        "for tr in markov_process.traces(start_dist):\n",
        "    finish_steps.append(steps_to_terminal_100(tr))\n",
        "    if len(finish_steps) >= N_iter:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Minimum number of steps: 6\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(f'Minimum number of steps: {min(finish_steps)}')\n",
        "plt.hist(finish_steps, bins=50)\n",
        "plt.xlabel(\"Number of steps to finish\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(f\"Snakes & Ladders finish-time distribution (N_iter={N_iter})\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$\\textbf{Comment}$: as a sanity check, let's look at the minimum number of steps we found when we did the sampling. We found the minimum to be 6 steps. This is possible with the following sequence 0 to 4 (then jump to 14), 14 to 20, 20 to 26, 26 to 28 (jump to 76), 76 to 80 (jump to 99), 99 to 100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtP62xp5JOPI"
      },
      "source": [
        "## Question 2: Markov Decision Processes (Led by Benjamin Ward)\n",
        "\n",
        "Consider an MDP with an infinite set of states ${S} = \\{1,2,3,\\ldots \\}$. The start state is $s=1$. Each state $s$ allows a continuous set of actions $a \\in [0,1]$. The transition probabilities are given by:\n",
        "$$\\mathbb{P}[s+1 \\mid s, a] = a, \\mathbb{P}[s \\mid s, a] = 1 - a \\text{ for all } s \\in S \\text{ for all } a \\in [0,1]$$\n",
        "For all states $s \\in {S}$ and actions $a \\in [0,1]$, transitioning from $s$ to $s+1$ results in a reward of $1-a$ and transitioning from $s$ to $s$ results in a reward of $1+a$. The discount factor $\\gamma=0.5$.\n",
        "\n",
        "### Problem Statement\n",
        "\n",
        "How can we derive a mathematical formulation for the value function and the optimal policy? And how do those functions change when we modify the action space?\n",
        "\n",
        "---\n",
        "\n",
        "### Subquestions\n",
        "\n",
        "#### Part (A): Optimal Value Function  \n",
        "\n",
        "Using the MDP Bellman Optimality Equation, calculate the Optimal Value Function $V^*(s)$ for all $s \\in {S}$. Given $V^*(s)$, what is the optimal action, $a^*$, that maximizes the optimal value function?\n",
        "\n",
        "---\n",
        "\n",
        "#### Part (B): Optimal Policy  \n",
        "\n",
        "Calculate an Optimal Deterministic Policy $\\pi^*(s)$ for all $s \\in {S}$.\n",
        "\n",
        "---\n",
        "\n",
        "#### Part (C): Changing the Action Space  \n",
        "\n",
        "Let's assume that we modify the action space such that instead of $a \\in [0,1]$ for all states, we restrict the action space to $a \\in \\left[0,\\frac{1}{s}\\right]$ for state $s$. This means that higher states have more restricted action spaces. How does this constraint affect:\n",
        "\n",
        "- The form of the Bellman optimality equation?\n",
        "- The optimal value function, $V^*(s)$?\n",
        "- The structure of the optimal policy, $\\pi^*(s)$?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2P6ZLj9JOPI"
      },
      "source": [
        "### Part (A) Answer\n",
        "The MDP Bellman Optimality equation given in the slides is $V^*(s)=\\max_{a\\in A}\\{R(s,a)+\\gamma\\sum_{s'\\in N}\\mathbb{P}(s,a,s')V^*(s')\\}$.\n",
        "\n",
        "We have $\\gamma=0.5$, $A=[0,1]$, $R(s,a)=a(1-a)+(1-a)(1+a) = 1+a-2a^2$, $\\mathbb{P}(s,a,s)=1-a$ and $\\mathbb{P}(s,a,s+1)=a$. Plugging all of this in the above formula gives:\n",
        "\\begin{align}\n",
        "V^*(s)=\\max_{a\\in [0,1]}\\{1-a-2a^2+0.5[(1-a)V^*(s)+aV^*(s+1)]\\}\n",
        "\\end{align}\n",
        "We then note that since there is no terminal state, and our actions are only \"stay\" or \"shift\", then $V^*(s)$ is a constant, which we note $v$. Plugging this in:\n",
        "\\begin{align}\n",
        "v &= \\max_{a\\in [0,1]}\\{1-a-2a^2+0.5[(1-a)v+av]\\} \\\\\n",
        "\\Rightarrow v &= 0.5v + \\max_{a\\in [0,1]}\\{1-a-2a^2+0.5\\} \\\\\n",
        "\\Rightarrow 0.5v &= \\max_{a\\in [0,1]}\\{1.5-a-2a^2\\} \\\\\n",
        "\\end{align}\n",
        "Which reaches a maximum for $a=\\frac{1}{4}$, giving $0.5v=1.5-0.25-0.125=1.125$, so $v=2.25$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnYSdwuXJOPJ"
      },
      "source": [
        "### Part (B) Answer\n",
        "We just showed in part (a) that the optimal deterministic policy was to choose $a=0.25$ always. So using the required notations, this means $\\forall s\\in \\mathbb{N}^*, \\pi^*(s)=0.25$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7DSJihCJOPJ"
      },
      "source": [
        "### Part (C) Answer\n",
        "\n",
        "#### Bellman Optimality Equation Change:\n",
        "We have only change the range of values $a$ can take (now we condition on s). So The Bellman Optimality Equation becomes:\n",
        "\\begin{align}\n",
        "V^*(s)=\\max_{a\\in [0,1/s]}\\{1-a-2a^2+0.5[(1-a)V^*(s)+aV^*(s+1)]\\}\n",
        "\\end{align}\n",
        "\n",
        "#### Optimal Value Function Change:\n",
        "Previously due to the translation invariance and no terminal states, we were able to conclude that the optimal value function was a constant independent of the state. Now this is no longer the case, because the optimization space is dependent on the state. However since the set of actions $A_s=[0,1/s]$ is decreasing in $s$, we can get an intuition for what is happening. This means that we have that $V^*(s)$ is decreasing in $s$. To show this, consider one \"player\" at $s$, one player at $s+1$. We let the player at $s+1$ choose their policy. Then since $\\forall t, A_t\\subseteq A_{t+1}$, the player at $s$ can just replicate the policy of the player at $s+1$. If we assume that they have the uniform values $U\\sim U[0,1]$ to decide the randomness, then we will always have the player which started at $s$ exactly one step behind the one who started at $s+1$. Since this holds for all policy that the player at $s+1$ could choose, we have $V^*(s)\\leq V*(s+1)$ so $V^*$ is decreasing. \n",
        "\n",
        "#### Optimal Policy Change:\n",
        "Since our Bellman Optimality Equation changed in such a way that we can't state that the value function is independent of the state and we can't calculate the explicit value of the value function which would have been needed to determine the optimal policy, we can no longer easily determine the fixed optimal policy. I was not able to analytically find the optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0q5xd7zJOPJ"
      },
      "source": [
        "## Question 3: Frog in a Pond (Led by Benjamin Ward)\n",
        "\n",
        "Consider an array of $n+1$ lilypads on a pond, numbered $0$ to $n$. A frog sits on a lilypad other than the lilypads numbered $0$ or $n$. When on lilypad $i$ ($1 \\leq i \\leq n-1$), the frog can croak one of two sounds: **A** or **B**.\n",
        "\n",
        "- If it croaks **A** when on lilypad $i$ ($1 \\leq i \\leq n-1$):\n",
        "  - It is thrown to lilypad $i-1$ with probability $\\frac{i}{n}$.\n",
        "  - It is thrown to lilypad $i+1$ with probability $\\frac{n-i}{n}$.\n",
        "  \n",
        "- If it croaks **B** when on lilypad $i$ ($1 \\leq i \\leq n-1$):\n",
        "  - It is thrown to one of the lilypads $0, \\ldots, i-1, i+1, \\ldots, n$ with uniform probability $\\frac{1}{n}$.\n",
        "\n",
        "A snake, perched on lilypad $0$, will eat the frog if it lands on lilypad $0$. The frog can escape the pond (and hence, escape the snake!) if it lands on lilypad $n$.\n",
        "\n",
        "### Problem Statement\n",
        "\n",
        "What should the frog croak when on each of the lilypads $1, 2, \\ldots, n-1$, in order to maximize the probability of escaping the pond (i.e., reaching lilypad $n$ before reaching lilypad $0$)?\n",
        "\n",
        "Although there are multiple ways to solve this problem, we aim to solve it by modeling it as a **Markov Decision Process (MDP)** and identifying the **Optimal Policy**.\n",
        "\n",
        "---\n",
        "\n",
        "### Subquestions\n",
        "\n",
        "#### Part (A): MDP Modeling\n",
        "\n",
        "Express the frog-escape problem as an MDP using clear mathematical notation by defining the following components:\n",
        "\n",
        "- **State Space**: Define the possible states of the MDP.\n",
        "- **Action Space**: Specify the actions available to the frog at each state.\n",
        "- **Transition Function**: Describe the probabilities of transitioning between states for each action.\n",
        "- **Reward Function**: Specify the reward associated with the states and transitions.\n",
        "\n",
        "---\n",
        "\n",
        "#### Part (B): Python Implementation\n",
        "\n",
        "There is starter code below to solve this problem programatically. Fill in each of the $6$ `TODO` areas in the code. As a reference for the transition probabilities and rewards, you can make use of the example in slide 16/31 from the following slide deck: https://github.com/coverdrive/technical-documents/blob/master/finance/cme241/Tour-MP.pdf.\n",
        "\n",
        "Write Python code that:\n",
        "\n",
        "- Models this MDP.\n",
        "- Solves the **Optimal Value Function** and the **Optimal Policy**.\n",
        "\n",
        "Feel free to use/adapt code from the textbook. Note, there are other libraries that are needed to actually run this code, so running it will not do anything. Just fill in the code so that it could run assuming that the other libraries are present.\n",
        "\n",
        "---\n",
        "\n",
        "#### Part (C): Visualization and Analysis\n",
        "\n",
        "What patterns do you observe for the **Optimal Policy** as you vary $n$ from $3$ to $25$? When the frog is on lilypad $13$ (with $25$ total), what action should the frog take? Is this action different than the action the frog should take if it is on lilypad $1$?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDERD0y-JOPJ"
      },
      "source": [
        "### Part (A) Answer\n",
        "\n",
        "#### State Space:  \n",
        "\n",
        "State Space = {0,1,...n}, Terminal States ={0,n}\n",
        "\n",
        "#### Action Space:  \n",
        "\n",
        "Action Space = {A,B} (only applies to non-terminal states, we consider that are the terminal states there are no actions)\n",
        "\n",
        "#### Transition Function:  \n",
        "\n",
        "Case of \"Croak A\" (for $i\\notin \\{0,n\\}$): $\\mathbb{P}[i-1\\lvert i,A] = \\frac{i}{n}, \\mathbb{P}[i+1\\lvert i,A] = \\frac{n-i}{n}$\n",
        "\n",
        "Case of \"Croak B\" (for $i\\notin \\{0,n\\}$): $\\forall k\\in [0,1,...,i-1,i+1,...,n]: \\mathbb{P}[k\\lvert i,B]=\\frac{1}{n}$\n",
        "\n",
        "For the terminal states: $\\mathbb{P}[0\\lvert 0,A]=\\mathbb{P}[0\\lvert 0,B]=1$ and $\\mathbb{P}[n\\lvert n,A]=\\mathbb{P}[n\\lvert n,B]=1$\n",
        "\n",
        "#### Reward Function:  \n",
        "\n",
        "We are \"rewarded\" if we escape i.e. reach $n$. So $R(n)=1$, and $\\forall k\\neq n, R(k)=0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVQ2qgCpJOPJ"
      },
      "source": [
        "### Part (B) Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "w6xyVJiyJOPJ"
      },
      "outputs": [],
      "source": [
        "MDPRefined = dict\n",
        "def get_lily_pads_mdp(n: int) -> MDPRefined:\n",
        "    data = {\n",
        "        i: {\n",
        "            'A': {\n",
        "                i - 1: i/n, # TODO: fill in with the correct transition probabilities\n",
        "                i + 1: (n-i)/n, # TODO: fill in with the correct transition probabilities\n",
        "            },\n",
        "            'B': {\n",
        "                j: 1/n for j in range(n+1) if j!=i # TODO: fill in with the correct transition probabilities\n",
        "            }\n",
        "        } for i in range(1, n)\n",
        "    }\n",
        "    # If 0: eaten for sure, if n: saved for sure\n",
        "    data[0] = {'A': {0: 1.0}, 'B': {0: 1.0}} # TODO: this is the initial state, so what would be the correct transition probabilities?\n",
        "    data[n] = {'A': {n: 1.0}, 'B': {n: 1.0}} # TODO: similarly, this is the terminal state, so what would be the correct transition probabilities?\n",
        "\n",
        "    gamma = 1.0\n",
        "    # I tweaked the return to just return the data dictionary to solve the problem\n",
        "    return data\n",
        "    # return MDPRefined(data, gamma)\n",
        "\n",
        "Mapping = dict\n",
        "def direct_bellman(n: int) -> Mapping[int, float]:\n",
        "    vf = [0.5] * (n + 1)\n",
        "    vf[0] = 0.\n",
        "    vf[n] = 0. \n",
        "    tol = 1e-8 # was 1e-8\n",
        "    epsilon = tol * 1e4\n",
        "    while epsilon >= tol:\n",
        "        old_vf = [v for v in vf]\n",
        "        for i in range(1, n):\n",
        "            # Action A \n",
        "            # Probabilities\n",
        "            p_left, p_right = i/n, (n-i)/n\n",
        "            # Reward\n",
        "            r_right = 1 if (i+1)==n else 0\n",
        "            qA = p_left*old_vf[i-1]+p_right*(old_vf[i+1]+r_right)\n",
        "            # Action B\n",
        "            qB = 0\n",
        "            for j in range(n+1):\n",
        "                if j==i:\n",
        "                    continue\n",
        "                r = 1 if j==n else 0\n",
        "                qB += (1/n)*(old_vf[j]+r)\n",
        "            # Maximise\n",
        "            vf[i] = max(qA,qB) # TODO: fill in with the Bellman update\n",
        "        epsilon = max(abs(old_vf[i] - v) for i, v in enumerate(vf))\n",
        "    return {v: f for v, f in enumerate(vf)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = direct_bellman(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 0.0, 1: 0.5714285703926619, 2: 0.7142857111779859, 3: 0.0}"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8G7rf9pJOPJ"
      },
      "source": [
        "### Part (C) Answer\n",
        "\n",
        "We find that whenever the frog is on lilypad 1, action B is preferred. In all other cases, action A is preferred. This makes intuitive sense. Action B creates a \"risk of ruin\" because there is a 1/n chance of ending on zero. Taking this risk is only worth it the frog is on lilypad 1. For example with n=25, we find that action B is preferred for lilypad 1, but for lilypad 13, action A is preferred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "def greedy_optimal_policy(n: int, vf_map: Dict[int, float]) -> Dict[int, str]:\n",
        "    vf = [vf_map[i] for i in range(n + 1)]\n",
        "    policy = {}\n",
        "    for i in range(1, n):\n",
        "        # Action A \n",
        "        # Probabilities\n",
        "        p_left, p_right = i/n, (n-i)/n\n",
        "        # Reward\n",
        "        r_right = 1 if (i+1)==n else 0\n",
        "        qA = p_left*vf[i-1]+p_right*(vf[i+1]+r_right)\n",
        "        # Action B\n",
        "        qB = 0\n",
        "        for j in range(n+1):\n",
        "            if j==i:\n",
        "                continue\n",
        "            r = 1 if j==n else 0\n",
        "            qB += (1/n)*(vf[j]+r)\n",
        "        policy[i] = 'A' if qA >= qB else 'B'\n",
        "    return policy\n",
        "\n",
        "def solve_frog_escape(n: int) -> Tuple[Dict[int, float], Dict[int, str]]:\n",
        "    # Optimal value function\n",
        "    vf = direct_bellman(n) \n",
        "    # Optimal policy\n",
        "    pi = greedy_optimal_policy(n, vf) \n",
        "    return vf, pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal action at lilypad 13 (n=25): A\n",
            "Optimal action at lilypad 1  (n=25): B\n"
          ]
        }
      ],
      "source": [
        "vf25, pi25 = solve_frog_escape(25)\n",
        "print(\"Optimal action at lilypad 13 (n=25):\", pi25[13])\n",
        "print(\"Optimal action at lilypad 1  (n=25):\", pi25[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Policy for 3: {1: 'B', 2: 'A'}\n",
            "Policy for 5: {1: 'B', 2: 'A', 3: 'A', 4: 'A'}\n",
            "Policy for 10: {1: 'B', 2: 'A', 3: 'A', 4: 'A', 5: 'A', 6: 'A', 7: 'A', 8: 'A', 9: 'A'}\n",
            "Policy for 15: {1: 'B', 2: 'A', 3: 'A', 4: 'A', 5: 'A', 6: 'A', 7: 'A', 8: 'A', 9: 'A', 10: 'A', 11: 'A', 12: 'A', 13: 'A', 14: 'A'}\n"
          ]
        }
      ],
      "source": [
        "policies = {}\n",
        "for n in [3,5,10,15]:\n",
        "    _, pi = solve_frog_escape(n)\n",
        "    policies[n] = pi\n",
        "    print(f\"Policy for {n}: {policies[n]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYo8Bs6OJOPK"
      },
      "source": [
        "## Question 4: Manual Value Iteration (Led by Benjamin Ward)\n",
        "\n",
        "Consider a simple MDP with ${S} = \\{s_1, s_2, s_3\\}, {T} = \\{s_3\\}, {A} = \\{a_1, a_2\\}$. The State Transition Probability function  \n",
        "$${P}: {N} \\times {A} \\times {S} \\rightarrow [0, 1]$$  \n",
        "is defined as:  \n",
        "$${P}(s_1, a_1, s_1) = 0.25, {P}(s_1, a_1, s_2) = 0.65, {P}(s_1, a_1, s_3) = 0.1$$  \n",
        "$${P}(s_1, a_2, s_1) = 0.1, {P}(s_1, a_2, s_2) = 0.4, {P}(s_1, a_2, s_3) = 0.5$$  \n",
        "$${P}(s_2, a_1, s_1) = 0.3, {P}(s_2, a_1, s_2) = 0.15, {P}(s_2, a_1, s_3) = 0.55$$  \n",
        "$${P}(s_2, a_2, s_1) = 0.25, {P}(s_2, a_2, s_2) = 0.55, {P}(s_2, a_2, s_3) = 0.2$$  \n",
        "\n",
        "The Reward Function  \n",
        "$${R}: {N} \\times {A} \\rightarrow \\mathbb{R}$$  \n",
        "is defined as:  \n",
        "$${R}(s_1, a_1) = 8.0, {R}(s_1, a_2) = 10.0$$  \n",
        "$${R}(s_2, a_1) = 1.0, {R}(s_2, a_2) = -1.0$$  \n",
        "\n",
        "Assume a discount factor of $\\gamma = 1$.\n",
        "\n",
        "### Problem Statement\n",
        "\n",
        "Your task is to determine an Optimal Deterministic Policy **by manually working out** (not with code) the first two iterations of the Value Iteration algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "### Subquestions\n",
        "\n",
        "#### Part (A): 2 Iterations\n",
        "\n",
        "1. Initialize the Value Function for each state to be its $\\max$ (over actions) reward, i.e., we initialize the Value Function to be $v_0(s_1) = 10.0, v_0(s_2) = 1.0, v_0(s_3) = 0.0$. Then manually calculate $q_k(\\cdot, \\cdot)$ and $v_k(\\cdot)$ from $v_{k - 1}(\\cdot)$ using the Value Iteration update, and then calculate the greedy policy $\\pi_k(\\cdot)$ from $q_k(\\cdot, \\cdot)$ for $k = 1$ and $k = 2$ (hence, 2 iterations).\n",
        "\n",
        "---\n",
        "\n",
        "#### Part (B): Argument\n",
        "\n",
        "1. Now argue that $\\pi_k(s_1)$ for $k > 2$ will be the same as $\\pi_2(s_1)$. *Hint*: You can make the argument by examining the structure of how you get $q_k(\\cdot, \\cdot)$ from $v_{k-1}(\\cdot)$. With this argument, there is no need to go beyond the two iterations you performed above, and so you can establish $\\pi_2(\\cdot)$ as an Optimal Deterministic Policy for this MDP.\n",
        "\n",
        "---\n",
        "\n",
        "#### Part (C): Policy Evaluation\n",
        "\n",
        "1. Using the policy $\\pi_2(\\cdot)$, compute the exact value function $V^{\\pi_2}(s)$ for all $s\\in S$.\n",
        "\n",
        "---\n",
        "\n",
        "#### Part (D): Sensitivity Analysis\n",
        "\n",
        "Assume the reward for $R(s_1, a_2)$ is modified to $11.0$ instead of $10.0$.\n",
        "\n",
        "1. Perform one iteration of Value Iteration starting from the initialized value function $v_0(s)$, where $v_0(s)$ remains the same as in the original problem.\n",
        "2. (IGNORE THIS) Determine whether this change impacts the Optimal Deterministic Policy $\\pi(\\cdot)$. If it does, explain why.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YtW9HzWJOPK"
      },
      "source": [
        "### Part (A) Answer\n",
        "\n",
        "We know that (for a discount factor of 1):\n",
        "\\begin{align}\n",
        "q_k(s,a) &= R(s,a)+\\sum_{s'}P(s,a,s')v_{k-1}(s') \\\\\n",
        "v_k(s)&=\\max_a q_k(s,a)\n",
        "\\end{align}\n",
        "Plugging in the numerical values given with an initialization of $v_0(s_1)=10$, $v_0(s_2)=0$ and $v_0(s_3)=0$ gives:\n",
        "\\begin{align}\n",
        "q_1(s_1,a_1) &= 8.0 + 0.25*10 + 0.65*1 + 0.1*0 = 11.15 \\\\\n",
        "q_1(s_2,a_1) &= 1.0 + 0.3*10 + 0.15*1 + 0.55*0 = 4.15 \\\\\n",
        "q_1(s_1,a_2) &= 10.0 + 0.1*10 + 0.4*1 + 0.5*0 = 11.4 \\\\\n",
        "q_1(s_2,a_2) &= -1.0 + 0.25*10 + 0.55*1 + 0.2*0 = 2.05\\\\\n",
        "\\end{align}\n",
        "So maximizing over actions gives:\n",
        "\\begin{align}\n",
        "v_1(s_1) &= 11.4 \\\\\n",
        "v_1(s_2) &= 4.15 \\\\\n",
        "v_1(s_3) &= 0 \\\\\n",
        "\\end{align}\n",
        "Therefore we have the greedy policy:\n",
        "\\begin{align}\n",
        "\\pi_1(s_1) &= a_2 \\text{ since 11.4>11.15} \\\\\n",
        "\\pi_1(s_2) &= a_1 \\text{ since 4.15>2.05}\n",
        "\\end{align}\n",
        "Let's now move forward to k=2, same procedure:\n",
        "\\begin{align}\n",
        "q_2(s_1,a_1) &= 8.0 + 0.25*11.4 + 0.65*4.15 + 0.1*0 = 13.5475 \\\\\n",
        "q_2(s_2,a_1) &= 1.0 + 0.3*11.4 + 0.15*4.15 + 0.55*0 = 5.0425  \\\\\n",
        "q_2(s_1,a_2) &= 10.0 + 0.1*11.4 + 0.4*4.15 + 0.5*0 = 12.80 \\\\\n",
        "q_2(s_2,a_2) &= -1.0 + 0.25*11.4 + 0.55*4.15 + 0.2*0 = 4.1325 \\\\\n",
        "\\end{align}\n",
        "So maximizing over actions gives:\n",
        "\\begin{align}\n",
        "v_2(s_1) &=  13.5475 \\\\\n",
        "v_2(s_2) &=  5.0425\\\\\n",
        "v_2(s_3) &=  0 \\\\\n",
        "\\end{align}\n",
        "Therefore we have the greedy policy:\n",
        "\\begin{align}\n",
        "\\pi_2(s_1) &= a_1 \\text{ since 13.5475>12.80} \\\\\n",
        "\\pi_2(s_2) &= a_1 \\text{ since 5.0425>4.1325}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BbZBLMyJOPK"
      },
      "source": [
        "### Part (B) Answer:  \n",
        "\n",
        "Let's prove that for all $k\\geq 2$, the $\\pi_k(s_1)=\\pi_2(s_1)=a_1$.\n",
        "We compute:\n",
        "\\begin{align}\n",
        "\\Delta_k = q_k(s_1,a_1)-q_k(s_1,a_2) &= R(s_1,a_1)+\\sum_{s'}P(s_1,a_2,s')v_{k-1}(s') - R(s_1,a_2)+\\sum_{s'}P(s,a_2,s')v_{k-1}(s') \\\\\n",
        "&= [R(s_1,a_1)-R(s_1,a_2)] + \\sum_{s'}[P(s_1,a_2,s')-P(s_1,a_2,s')]v_{k-1}(s') \\\\\n",
        "&= [8-10] + (0.25-0.1)v_{k-1}(s_1)+ (0.65-0.4)v_{k-1}(s_2) + (0.1-0.5)v_{k-1}(s_3) \\\\\n",
        "&= -2 + 0.15*v_{k-1}(s_1) + 0.25*v_{k-1}(s_2) + 0 \n",
        "\\end{align}\n",
        "In particular, since $\\pi_2(s_1)=a_1$, this means $\\Delta_2>0$.\n",
        "\n",
        "We now use the monotonicity property of the Bellman operator (slide 19/36), which gives that if $v_k(s_i)\\leq v_{k+1}(s_i)$. \n",
        "\n",
        "In particular, applied to $s_1,s_2$, this means that $v_k(s_1)<=v_{k+1}(s_1), v_k(s_2)<=v_{k+1}(s_2)$. \n",
        "\n",
        "Now note that $\\Delta_k=-2 + 0.15*v_k(s_1) + 0.25*v_k(s_2)$, so $\\Delta_k$ is an increasing function of $v_{k-1}(s_1)$ and of $v_k{k-1}(s_2)$, so $\\Delta_k\\geq \\Delta_{k-1}$, so $\\Delta_k$ is an increasing function of k. \n",
        "\n",
        "Since $\\Delta_2>0$, we have $\\forall k\\geq 2, \\Delta_k>=\\Delta_2>0$ i.e. $q_k(s_1,a_1)>q_k(s_1,a_2)$, so $\\pi_k(s_1)=a_1$, which is what we wanted to prove.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWP2WGIJJOPK"
      },
      "source": [
        "### Part (C) Answer:  \n",
        "\n",
        "Let's calculate the value function using $\\pi_2$.  We showed that $\\pi_2(s_1)=a_1$, $\\pi_2(s_2)=a_1$, and since $s_3$ is terminal, $V^{\\pi_2}(s_3)=0$. Using $\\gamma=1$ in the Bellman equation gives:\n",
        "\\begin{align}\n",
        "V^{\\pi_2}(s_1) &= R(s_1,a_1)+\\sum_{s'}P(s_1,a_1,s')V^{\\pi_2}(s') \\\\\n",
        "&= 8+0.25V^{\\pi_2}(s_1)+0.65V^{\\pi_2}(s_2) \\\\\n",
        "\\Rightarrow 0.75V^{\\pi_2}(s_1)- 0.65V^{\\pi_2}(s_2) &= 8 \\\\\n",
        "V^{\\pi_2}(s_2) &= R(s_2,a_1)+\\sum_{s'}P(s_2,a_1,s')V^{\\pi_2}(s') \\\\\n",
        "&= 1+0.3V^{\\pi_2}(s_1)+0.15V^{\\pi_2}(s_2) \\\\\n",
        "\\Rightarrow -0.3V^{\\pi_2}(s_1)+ 0.85V^{\\pi_2}(s_2) &= 1\n",
        "\\end{align}\n",
        "So we need to solve the 2 equation 2 unknown system:\n",
        "\\begin{align}\n",
        "0.75V^{\\pi_2}(s_1)- 0.65V^{\\pi_2}(s_2) &= 8 \\\\\n",
        "-0.3V^{\\pi_2}(s_1)+ 0.85V^{\\pi_2}(s_2) &= 1\n",
        "\\end{align}\n",
        "Which gives $V^{\\pi_2}(s_1)=\\frac{2980}{177}\\approx 16.84$ and $V^{\\pi_2}(s_2)=\\frac{420}{50}\\approx 7.12$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjdLaazcJOPK"
      },
      "source": [
        "### Part (D) Answer\n",
        "\n",
        "#### Value Iteration:  \n",
        "\n",
        "We now assume the reward for $R(s_1, a_2)$ is modified to $11.0$ instead of $10.0$. We do one value-iteration loop.\n",
        "\\begin{align}\n",
        "q_1(s_1,a_1) &= 8.0 + 0.25*10 + 0.65*1 + 0.1*0 = 11.15 \\\\\n",
        "q_1(s_2,a_1) &= 1.0 + 0.3*10 + 0.15*1 + 0.55*0 = 4.15 \\\\\n",
        "q_1(s_1,a_2) &= 11.0 + 0.1*10 + 0.4*1 + 0.5*0 = 12.4 \\\\\n",
        "q_1(s_2,a_2) &= -1.0 + 0.25*10 + 0.55*1 + 0.2*0 = 2.05\\\\\n",
        "\\end{align}\n",
        "So maximizing over actions gives:\n",
        "\\begin{align}\n",
        "v_1(s_1) &= 12.4 \\\\\n",
        "v_1(s_2) &= 4.15 \\\\\n",
        "v_1(s_3) &= 0 \\\\\n",
        "\\end{align}\n",
        "Therefore we have the (same as before) greedy policy:\n",
        "\\begin{align}\n",
        "\\pi_1(s_1) &= a_2 \\text{ since 12.4>11.15} \\\\\n",
        "\\pi_1(s_2) &= a_1 \\text{ since 4.15>2.05}\n",
        "\\end{align}\n",
        "\n",
        "#### Optimal Deterministic Policy:  \n",
        "\n",
        "(IGNORED FOLLOWING ED POST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef0b2Q4QJOPK"
      },
      "source": [
        "## Question 5: Fixed-Point and Policy Evaluation True/False Questions (Led by Benjamin Ward)\n",
        "\n",
        "### Recall Section: Key Formulas and Definitions\n",
        "\n",
        "#### Bellman Optimality Equation\n",
        "The Bellman Optimality Equation for state-value functions is:\n",
        "$$\n",
        "V^*(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s, a, s') V^*(s') \\right].\n",
        "$$\n",
        "For action-value functions:\n",
        "$$\n",
        "Q^*(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s, a, s') \\max_{a'} Q^*(s', a').\n",
        "$$\n",
        "\n",
        "#### Contraction Property\n",
        "The Bellman Policy Operator $B^\\pi$ is a contraction under the $L^\\infty$-norm:\n",
        "$$\n",
        "\\|B^\\pi(X) - B^\\pi(Y)\\|_\\infty \\leq \\gamma \\|X - Y\\|_\\infty.\n",
        "$$\n",
        "This guarantees convergence to a unique fixed point.\n",
        "\n",
        "#### Policy Iteration\n",
        "Policy Iteration alternates between:\n",
        "1. **Policy Evaluation**: Compute $V^\\pi$ for the current policy $\\pi$.\n",
        "2. **Policy Improvement**: Generate a new policy $\\pi'$ by setting:\n",
        "   $$\n",
        "   \\pi'(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s, a, s') V^\\pi(s') \\right].\n",
        "   $$\n",
        "\n",
        "#### Discounted Return\n",
        "The discounted return from time step $t$ is:\n",
        "$$\n",
        "G_t = \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i,\n",
        "$$\n",
        "where $\\gamma \\in [0, 1)$ is the discount factor.\n",
        "\n",
        "### True/False Questions (Provide Justification)\n",
        "\n",
        "1. **True/False**: If $Q^\\pi(s, a) = 5$, $P(s, a, s') = 0.5$ for $s' \\in \\{s_1, s_2\\}$, and the immediate reward $R(s, a)$ increases by $2$, the updated action-value function $Q^\\pi(s, a)$ also increases by $2$.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "2. **True/False**: For a discount factor $\\gamma = 0.9$, the discounted return for rewards $R_1 = 5, R_2 = 3, R_3 = 1$ is greater than $6$.\n",
        "\n",
        "---\n",
        "\n",
        "3. **True/False**: The Bellman Policy Operator $B^\\pi(V) = R^\\pi + \\gamma P^\\pi \\cdot V$ satisfies the contraction property for all $\\gamma \\in [0, 1)$, ensuring a unique fixed point.\n",
        "\n",
        "---\n",
        "\n",
        "4. **True/False**: In Policy Iteration, the Policy Improvement step guarantees that the updated policy $\\pi'$ will always perform strictly better than the previous policy $\\pi$.\n",
        "\n",
        "---\n",
        "\n",
        "5. **True/False**: If $Q^\\pi(s, a) = 10$ for all actions $a$ in a state $s$, then the corresponding state-value function $V^\\pi(s) = 10$, regardless of the policy $\\pi$.\n",
        "\n",
        "---\n",
        "\n",
        "6. **True/False**: The discounted return $G_t = \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i$ converges to a finite value for any sequence of bounded rewards if $\\gamma < 1$.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KahXGmEJOPK"
      },
      "source": [
        "### Answers (Provide justification, brief explanations are fine)\n",
        "\n",
        "#### Question 1:  \n",
        "\n",
        "<span style=\"color:red\">*fill in*</span>\n",
        "\n",
        "#### Question 2:  \n",
        "\n",
        "<span style=\"color:red\">*fill in*</span>\n",
        "\n",
        "#### Question 3:  \n",
        "\n",
        "<span style=\"color:red\">*fill in*</span>\n",
        "\n",
        "#### Question 4:  \n",
        "\n",
        "<span style=\"color:red\">*fill in*</span>\n",
        "\n",
        "#### Question 5:  \n",
        "\n",
        "<span style=\"color:red\">*fill in*</span>\n",
        "\n",
        "#### Question 6:  \n",
        "\n",
        "<span style=\"color:red\">*fill in*</span>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "75099216e894b6f9152c3594a0e30c5ebc76b500c33c0ebb90f919eed94b7716"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
