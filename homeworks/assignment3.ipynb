{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2026) - Assignment 3\n",
    "\n",
    "**Due: Friday, February 27 @ 11:59 PM PST on Gradescope.**\n",
    "\n",
    "Assignment instructions:\n",
    "- Make sure each of the subquestions have answers\n",
    "- Ensure that group members indicate which problems they're in charge of\n",
    "- Show work and walk through your thought process where applicable\n",
    "- Empty code blocks are for your use, so feel free to create more under each section as needed\n",
    "- Document code with light comments (i.e. 'this function handles visualization')\n",
    "\n",
    "Submission instructions:\n",
    "- When complete, fill out your publicly available GitHub repo file URL and group members below, then export or print this .ipynb file to PDF and upload the PDF to Gradescope.\n",
    "\n",
    "*Link to this ipynb file in your public GitHub repo (replace below URL with yours):* \n",
    "\n",
    "https://github.com/ward-benjamin/CME241_RL/blob/master/homeworks/assignment3.ipynb\n",
    "\n",
    "*Group members (replace below names with people in your group):* \n",
    "- Benjamin Ward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize_scalar\n",
    "from typing import Iterable, Iterator, Mapping, TypeVar, Callable, Sequence, Tuple, Dict\n",
    "from rl.distribution import Categorical\n",
    "from rl.markov_process import FiniteMarkovRewardProcess\n",
    "from rl.chapter10.prediction_utils import compare_td_and_mc\n",
    "X = TypeVar('X')\n",
    "S = TypeVar('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Milk Vendor Optimization Problem (Led by Benjamin Ward)\n",
    "\n",
    "You are a milk vendor and your task is to bring to your store a supply (denoted $S \\in \\mathbb{R}$) of milk volume in the morning that will give you the best profits. You know that the demand for milk throughout the course of the day is a probability distribution function $f$ (for mathematical convenience, assume people can buy milk in volumes that are real numbers, hence milk demand $x \\in \\mathbb{R}$ is a continuous variable with a probability density function). \n",
    "\n",
    "For every extra gallon of milk you carry at the end of the day (supply $S$ exceeds random demand $x$), you incur a cost of $h$ (effectively the wasteful purchases amounting to the difference between your purchase price and the end-of-day discount disposal price since you are not allowed to sell the same milk the next day). For every gallon of milk that a customer demands that you don’t carry (random demand $x$ exceeds supply $S$), you incur a cost of $p$ (effectively the missed sales revenue amounting to the difference between your sales price and purchase price). \n",
    "\n",
    "Your task is to identify the optimal supply $S$ that minimizes your **Expected Cost** $g(S)$, given by:\n",
    "\n",
    "$$\n",
    "g_1(S) = \\mathbb{E}[\\max(x - S, 0)] = \\int_{S}^{\\infty} (x - S) \\cdot f(x) \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_2(S) = \\mathbb{E}[\\max(S - x, 0)] = \\int_{-\\infty}^{S} (S - x) \\cdot f(x) \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "g(S) = p \\cdot g_1(S) + h \\cdot g_2(S)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): Analytical Optimization\n",
    "\n",
    "1. **Derive the first-order condition (FOC)** for minimizing the expected cost $g(S)$.\n",
    "2. **Solve the FOC** to express the optimal supply $S^*$ in terms of the given parameters: $p$, $h$, and the demand distribution $f(x)$. (*Hint*: Pay attention to the balance between the costs of overstocking and understocking)\n",
    "\n",
    "3. **Interpretation**: Provide an interpretation of the condition you derived. What does the balance between $p$ and $h$ imply about the optimal supply $S^*$?\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Special Case Analysis\n",
    "\n",
    "1. Consider the case where the demand $x$ follows an **exponential distribution** with parameter $\\lambda > 0$. That is, $f(x) = \\lambda e^{-\\lambda x}$ for $x \\geq 0$.\n",
    "    - Derive an explicit expression for the optimal supply $S^*$.\n",
    "    \n",
    "2. Consider the case where the demand $x$ follows a **normal distribution** with mean $\\mu$ and variance $\\sigma^2$, i.e., $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$. \n",
    "    - Set up the integral for $g(S)$ and describe how it relates to the **cumulative distribution function (CDF)** of the normal distribution.\n",
    "    - Provide an interpretation of how changes in $\\mu$ and $\\sigma$ influence the optimal $S^*$. \n",
    "\n",
    "---\n",
    "\n",
    "### Part (C): Framing as a Financial Options Problem\n",
    "\n",
    "1. Frame the milk vendor’s problem as a **portfolio of call and put options**:\n",
    "    - Identify the analog of the “strike price” and “underlying asset.”\n",
    "    - Explain which part of the cost function $g_1(S)$ or $g_2(S)$ corresponds to a call option and which part to a put option.\n",
    "    - What do $p$ and $h$ represent in this options framework?\n",
    "\n",
    "2. Explain how this framing could be used to derive the optimal supply $S^*$ if solved using financial engineering concepts.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (D): Numerical Solution and Simulation\n",
    "\n",
    "1. **Numerical Solution**: Write a Python function that numerically estimates the optimal $S^*$ using an iterative search or numerical optimization method. \n",
    "\n",
    "2. **Simulation**: Generate random samples of milk demand from an exponential distribution and simulate the total costs for different values of $S$. Plot the costs against $S$ and visually identify the optimal $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Let's first note that since $f$ is a density, if we note $F$ its primitive, then $F(-\\infty)=0$ and $F(+\\infty)=1$. We then have using Leibniz's rule:\n",
    "\\begin{align}\n",
    "g(S)&=pg_1(S)+hg_2(S)= p\\int_{S}^{\\infty} (x - S) \\cdot f(x) \\, dx + h\\int_{-\\infty}^{S} (S - x) \\cdot f(x) \\, dx \\\\\n",
    "\\Rightarrow g'(S)&=p(0+0+\\int_S^{\\infty}(-f(x))\\, dx) + h(0 + 0 + \\int_{-\\infty}^S f(x)\\, dx) \\\\\n",
    "&= -p(F(+\\infty)-F(S))+h(F(S)-F(-\\infty)) \\\\\n",
    "&= -p+(p+h)F(S)\n",
    "\\end{align}\n",
    "So the FOC for minimizing the expected cost $g(S)$, that is $g'(S)=0$ can be written as $-p+(p+h)F(S)=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In 1. we found the optimal supply $S^*$ must satisfy the FOC $-p+(p+h)F(S^*)=0$, therefore $S^*=F^{-1}(\\frac{p}{p+h})$ where $F(S)=\\int_{-\\infty}^Sf(x)\\,dx$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In 2, we found that $S^*$ must satisfy $F(S^*)=\\frac{p}{p+h}$. In words, this means that we choose $S^*$ such that the probability that demand is less than or equal to $S^*$ is equal to $\\frac{p}{p+h}$. If $p$ is high (cost of \"missed\" sales), then we increase the supply of milk we bring to the store, which makes intuitive sense. If $h$ is high (cost of holding inventory), then we reduce the supply of milk we bring to the store, which also makes intuitive sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In Part (A), we found that the optimal supply $S^*$ satisfies $F(S^*)=\\frac{p}{p+h}$. For an exponential distribution of parameter $\\lambda>0$, we have $F(S)=\\int_{0}^S \\lambda e^{-\\lambda x}dx=1-e^{-\\lambda S}$, such that we now have $1-e^{-\\lambda S^*}=\\frac{p}{p+h}$, so:\n",
    "\\begin{align}\n",
    "e^{-\\lambda S^*} &= 1-\\frac{p}{p+h} \\\\\n",
    "&= \\frac{h}{h+p} \\\\\n",
    "\\Rightarrow S^* &= \\frac{1}{-\\lambda}\\log(\\frac{h}{h+p}) \\\\\n",
    "&= \\frac{1}{\\lambda}\\log(\\frac{h+p}{h})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Suppose that $x\\sim \\mathcal{N}(\\mu,\\sigma^2)$. We note $\\phi_{\\mu,\\sigma}$ the PDF and $\\Phi_{\\mu,\\sigma}$ the CDF. To lighten notations we note $\\phi$ and $\\Phi$ the corresponding PDF and CDF for the standard normal. Then using the formula given for $g(S)$:\n",
    "\\begin{align}\n",
    "g(S)&= p\\int_{S}^{\\infty} (x - S) \\cdot f(x) \\, dx + h\\int_{-\\infty}^{S} (S - x) \\cdot f(x) \\, dx \\\\\n",
    "&= p\\int_{S}^{\\infty} (x - S) \\cdot \\phi_{\\mu,\\sigma}(x) \\, dx + h\\int_{-\\infty}^{S} (S - x) \\cdot \\phi_{\\mu,\\sigma}(x) \\, dx\n",
    "\\end{align}\n",
    "Let's get back to our implicit equation for $S^*$, namely $F(S^*)=\\frac{p}{p+h}$. Since $x\\sim \\mathcal{N}(\\mu,\\sigma^2)$, then $F(S)=\\Phi(\\frac{S-\\mu}{\\sigma})$, such that our equation becomes:\n",
    "\\begin{align}\n",
    "\\Phi(\\frac{S^*-\\mu}{\\sigma}) &= \\frac{p}{p+h} \\\\\n",
    "\\Rightarrow S^* &= \\mu + \\sigma\\Phi^{-1}(\\frac{p}{p+h})\n",
    "\\end{align}\n",
    "This means that for every unit of increase in $\\mu$, the optimal supply $S^*$ increase by one unit. \n",
    "Further, for every one unit increase in $\\sigma$, $S^*$ increase by $\\Phi^{-1}(\\frac{p}{p+h})$. We have 3 cases. \n",
    "- First if $p<h$, then $\\frac{p}{p+h}<1/2$, so $\\Phi^{-1}(\\frac{p}{p+h})<0$, so an increase in $\\sigma$ leads to a decrease in the optimal $S^*$. \n",
    "- Second if $p=h$, then $\\frac{p}{p+h}=1/2$, so $\\Phi^{-1}(\\frac{p}{p+h})=0$, so $S^*$ is independent of $\\sigma$. \n",
    "- Third if $p>h$, then $\\frac{p}{p+h}>1/2$, so $\\Phi^{-1}(\\frac{p}{p+h})>0$, so an increase in $\\sigma$ leads to an increase in the optimal $S^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We notice that $g_1(S) = \\mathbb{E}[\\max(x - S, 0)]$ which is the payoff of a call with underlying asset x and strike price S, and similarly $g_2(S) = \\mathbb{E}[\\max(S - x, 0)]$ is the payoff of a put with underlying asset x and strike price S. So  $g(S) = p \\cdot g_1(S) + h \\cdot g_2(S)$ is the payoff of a portfolio consisting of $p$ units of the previously described call, and $h$ units of the previously described put. Since we want to minimize $g(S)$, our \"actual\" portfolio is \"opposite\", namely we are short $p$ units of the call and short $h$ units of the put."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We computed the expression for the optimal supply $S^*$ in Part (A) using the FOC for $g(S)$, which can be re-written as:\n",
    "\\begin{align}\n",
    "pg_1'(S^*)&=-hg_2'(S^*) \\\\\n",
    "\\Rightarrow p(1-F(S^*))&= hF(S^*) \\\\\n",
    "\\Rightarrow p\\mathbb{P}(x>S^*) &= hP(x\\leq S^*)\n",
    "\\end{align} \n",
    "This means that us (we are selling the put and the calls), choose $S^*$ such that the probability of the call finishing in the money is equal to the probability of the put finishing in the money, weighted by our exposure (number of units of calls and puts we sold)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (D) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fill in the code below, and then run the code in the next cell for the simulation\n",
    "'''\n",
    "\n",
    "# simulation parameters\n",
    "p = 5  # Cost of understocking (per unit)\n",
    "h = 2  # Cost of overstocking (per unit)\n",
    "lambda_param = 1.0  # Parameter for exponential distribution\n",
    "\n",
    "# Probability density function for demand\n",
    "def demand_pdf(x):\n",
    "    return lambda_param * np.exp(-lambda_param*x)\n",
    "\n",
    "# Cumulative distribution function for demand\n",
    "def demand_cdf(x):\n",
    "    return max(0,1-np.exp(-lambda_param*x))\n",
    "\n",
    "# Expected cost function g(S)\n",
    "def expected_cost(S):\n",
    "    # g1(S): Understocking cost\n",
    "    g1 = p*(np.exp(-lambda_param*S)/lambda_param)\n",
    "    \n",
    "    # g2(S): Overstocking cost (integral using CDF)\n",
    "    g2 = h*(S-(1/lambda_param)*np.exp(-lambda_param*S)/lambda_param)\n",
    "    \n",
    "    return g1 + g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.4054660494382044)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = minimize_scalar(expected_cost, bounds=(0, 10), method='bounded')\n",
    "optimal_S = result.x\n",
    "\n",
    "# Simulation of costs\n",
    "S_values = np.linspace(0, 10, 500)\n",
    "costs = [expected_cost(S) for S in S_values]\n",
    "\n",
    "# Plotting the costs against S\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(S_values, costs, label=\"Expected Cost $g(S)$\", color='blue')\n",
    "plt.axvline(optimal_S, color='red', linestyle='--', label=f\"Optimal $S^* \\\\approx {optimal_S:.2f}$\")\n",
    "plt.xlabel(\"Supply $S$\")\n",
    "plt.ylabel(\"Expected Cost $g(S)$\")\n",
    "plt.title(\"Expected Cost $g(S)$ vs. Supply $S$ (Exponential Demand)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "optimal_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Car Sales (Led By: Benjamin Ward)\n",
    "\n",
    "You must sell your car within a finite window of $N$ days. At the beginning of each day $t \\in \\{1, \\dots, N\\}$, you receive a single offer $X_t$ from a dealership, where $\\{X_t\\}$ are i.i.d. draws from a known continuous distribution $Q$ supported on $[m, M]$, with $0 < m < M$.\n",
    "\n",
    "After observing $X_t$, you must immediately choose whether to **accept** or **reject** the offer:\n",
    "\n",
    "- If you **accept** on day $t$, you sell the car immediately and receive payoff $X_t$. The process then terminates.\n",
    "- If you **reject** on day $t < N$, you keep the car, pay a parking cost $c \\ge 0$ at the end of that day, and proceed to day $t+1$.\n",
    "- On day $N$, you must accept the offer (i.e., rejecting is not allowed).\n",
    "\n",
    "Your objective is to maximize the expected **net payoff**, defined as the sale price minus the total parking costs paid before the sale.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): MDP Modeling\n",
    "\n",
    "Model this problem as a finite-horizon **Markov Decision Process (MDP)**. Clearly specify:\n",
    "\n",
    "1. **States**\n",
    "2. **Actions**\n",
    "3. **Rewards**\n",
    "4. **State-transition probabilities**\n",
    "5. **Discount factor** (use $\\gamma = 1$)\n",
    "\n",
    "Also state whether this MDP is finite- or infinite-horizon, episodic or continuing, and whether it is time-homogeneous.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Structure of the Optimal Policy\n",
    "\n",
    "Let $V_t$ denote the optimal value function when there are $t$ days remaining (before observing the offer).\n",
    "\n",
    "1. Write down the Bellman recursion for $V_t$.\n",
    "2. Show that the optimal policy is a **time-dependent threshold policy**: that is, there exists a reservation price $r_t$ such that it is optimal to accept an offer $x$ if and only if $x \\ge r_t$.\n",
    "3. Express the threshold $r_t$ in terms of $V_{t-1}$ and $c$.\n",
    "\n",
    "(You may leave expectations with respect to $Q$ written in integral or expectation form. A closed-form solution for general $Q$ is not required.)\n",
    "\n",
    "---\n",
    "\n",
    "### Part (C): Special Case (No Integrals)\n",
    "\n",
    "Now consider the special case where $c = 0$ and $Q = \\text{Uniform}[m, M]$.\n",
    "\n",
    "1. Write the recursion for $V_t$ explicitly.\n",
    "2. Provide the optimal policy in as closed-form a way as possible. Your policy may depend on recursively defined coefficients, but your final expressions should contain **no integrals**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model this problem as a finite-horizon **Markov Decision Process (MDP)**. Clearly specify:\n",
    "\n",
    "1. **States**\n",
    "2. **Actions**\n",
    "3. **Rewards**\n",
    "4. **State-transition probabilities**\n",
    "5. **Discount factor** (use $\\gamma = 1$)\n",
    "\n",
    "Also state whether this MDP is finite- or infinite-horizon, episodic or continuing, and whether it is time-homogeneous.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $V_t$ denote the optimal value function when there are $t$ days remaining (before observing the offer).\n",
    "\n",
    "1. Write down the Bellman recursion for $V_t$.\n",
    "2. Show that the optimal policy is a **time-dependent threshold policy**: that is, there exists a reservation price $r_t$ such that it is optimal to accept an offer $x$ if and only if $x \\ge r_t$.\n",
    "3. Express the threshold $r_t$ in terms of $V_{t-1}$ and $c$.\n",
    "\n",
    "(You may leave expectations with respect to $Q$ written in integral or expectation form. A closed-form solution for general $Q$ is not required.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the special case where $c = 0$ and $Q = \\text{Uniform}[m, M]$.\n",
    "\n",
    "1. Write the recursion for $V_t$ explicitly.\n",
    "2. Provide the optimal policy in as closed-form a way as possible. Your policy may depend on recursively defined coefficients, but your final expressions should contain **no integrals**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Constrained Consumption (Led By: Benjamin Ward)\n",
    "\n",
    "Consider the following discrete-time MDP for constrained consumption. At $t = 0$, the agent is given a finite amount $x_0 \\in \\mathbb{R}^+$ of a resource. In each time period, the agent can choose to consume any amount of the resource, with the consumption denoted as $c \\in [0, x]$ where $x$ is the amount of the resource remaining at the start of the time period. This consumption results in a reduction of the resource at the start of the next time period:  \n",
    "$$x' = x - c.$$  \n",
    "\n",
    "Consuming a quantity $c$ of the resource provides a utility of consumption equal to $U(c)$, and we adopt the **CRRA utility function**:  \n",
    "$$\n",
    "U(c) = \\frac{c^{1 - \\gamma}}{1 - \\gamma}, \\quad (\\gamma > 0, \\gamma \\neq 1)\n",
    "$$\n",
    "\n",
    "Our goal is to maximize the aggregate discounted utility of consumption until the resource is completely consumed. We assume a discount factor of $\\beta \\in (0, 1)$ when discounting the utility of consumption over any single time period. Assume parameters are such that the value function is finite.\n",
    "\n",
    "We model this as a **discrete-time, continuous-state-space, continuous-action-space, stationary, deterministic MDP**, and so our goal is to solve for the **Optimal Value Function** and associated **Optimal Policy**, which will give us the optimal consumption trajectory of the resource. Since this is a stationary MDP, the **State** is simply the amount $x$ of the resource remaining at the start of a time period. The **Action** is the consumption quantity $c$ in that time period. The **Reward** for a time period is $U(c)$ when the consumption in that time period is $c$. The discount factor over each single time period is $\\beta$.\n",
    "\n",
    "We assume that the **Optimal Policy** is given by:  \n",
    "$$\n",
    "c^* = \\theta^* \\cdot x \\quad \\text{for some } \\theta^* \\in [0, 1].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): Closed-form Expression for $V_\\theta(x)$\n",
    "\n",
    "Our first step is to consider a fixed deterministic policy, given by:  \n",
    "$$c = \\theta \\cdot x \\quad \\text{for some fixed } \\theta \\in [0, 1].$$  \n",
    "Derive a closed-form expression for the **Value Function** $V_\\theta(x)$ for a fixed deterministic policy, given by $c = \\theta \\cdot x$. Specifically, you need to express $V_\\theta(x)$ in terms of $\\beta$, $\\gamma$, $\\theta$, and $x$.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Solving for $\\theta^*$\n",
    "\n",
    "Use this closed-form expression for $V_\\theta(x)$ to solve for the $\\theta^*$ which maximizes $V_\\theta(x)$ (thus fetching us the **Optimal Policy** given by $c^* = \\theta^* \\cdot x$).\n",
    "\n",
    "---\n",
    "\n",
    "### Part (C): Expression for $V^*(x)$\n",
    "\n",
    "Use this expression for $\\theta^*$ to obtain an expression for the **Optimal Value Function** $V^*(x)$ in terms of only $\\beta$, $\\gamma$, and $x$.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (D): Validation of the Bellman Equation\n",
    "\n",
    "Validate that the **Optimal Policy** (derived in part B) and **Optimal Value Function** (derived in part C) satisfy the **Bellman Optimality Equation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (D) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Double Q-Learning (Led By: Benjamin Ward)\n",
    "\n",
    "It is known that **Q-Learning** can suffer from maximization bias during finite-sample training. In this problem, we consider a modification of tabular Q-Learning called **Double Q-Learning**, which reduces this bias by decoupling action selection and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Algorithm 1: Double Q-Learning**\n",
    "\n",
    "**Initialize** $Q_1(s,a)$ and $Q_2(s,a)$ for all $s \\in \\mathcal{S}$, $a \\in \\mathcal{A}$  \n",
    "**yield** estimate of $Q$\n",
    "\n",
    "**while** True **do**  \n",
    "&emsp; **select** initial state $s_0$, set $t = 0$  \n",
    "&emsp; **while** $s_t$ is non-terminal **do**  \n",
    "\n",
    "&emsp;&emsp; **select** $a_t$ using $\\epsilon$-greedy with respect to  \n",
    "&emsp;&emsp; $\\displaystyle \\pi(s) = \\arg\\max_a \\big( Q_1(s,a) + Q_2(s,a) \\big)$  \n",
    "\n",
    "&emsp;&emsp; **observe** $(r_t, s_{t+1})$  \n",
    "\n",
    "&emsp;&emsp; **with probability 0.5 update $Q_1$:**\n",
    "\n",
    "&emsp;&emsp;&emsp; Let  \n",
    "&emsp;&emsp;&emsp; $\\displaystyle a^* = \\arg\\max_a Q_1(s_{t+1}, a)$  \n",
    "\n",
    "&emsp;&emsp;&emsp; Update  \n",
    "&emsp;&emsp;&emsp; $\\displaystyle \n",
    "Q_1(s_t, a_t) \\leftarrow Q_1(s_t, a_t)\n",
    "+ \\alpha \\Big(\n",
    "r_t + \\gamma Q_2(s_{t+1}, a^*)\n",
    "- Q_1(s_t, a_t)\n",
    "\\Big)\n",
    "$\n",
    "\n",
    "&emsp;&emsp; **otherwise update $Q_2$:**\n",
    "\n",
    "&emsp;&emsp;&emsp; Let  \n",
    "&emsp;&emsp;&emsp; $\\displaystyle a^* = \\arg\\max_a Q_2(s_{t+1}, a)$  \n",
    "\n",
    "&emsp;&emsp;&emsp; Update  \n",
    "&emsp;&emsp;&emsp; $\\displaystyle \n",
    "Q_2(s_t, a_t) \\leftarrow Q_2(s_t, a_t)\n",
    "+ \\alpha \\Big(\n",
    "r_t + \\gamma Q_1(s_{t+1}, a^*)\n",
    "- Q_2(s_t, a_t)\n",
    "\\Big)\n",
    "$\n",
    "\n",
    "&emsp;&emsp; $t \\leftarrow t + 1$  \n",
    "&emsp;&emsp; $s_t \\leftarrow s_{t+1}$  \n",
    "\n",
    "**yield** estimate $Q_1 + Q_2$\n",
    "\n",
    "---\n",
    "\n",
    "### **Algorithm 2: Q-Learning**\n",
    "\n",
    "**Initialize** $Q(s,a)$ for all $s \\in \\mathcal{S}$, $a \\in \\mathcal{A}$  \n",
    "**yield** $Q$\n",
    "\n",
    "**while** True **do**  \n",
    "&emsp; **select** initial state $s_0$, set $t = 0$  \n",
    "&emsp; **while** $s_t$ is non-terminal **do**  \n",
    "\n",
    "&emsp;&emsp; **select** $a_t$ using $\\epsilon$-greedy with respect to  \n",
    "&emsp;&emsp; $\\displaystyle \\pi(s) = \\arg\\max_a Q(s,a)$  \n",
    "\n",
    "&emsp;&emsp; **observe** $(r_t, s_{t+1})$  \n",
    "\n",
    "&emsp;&emsp; Update  \n",
    "&emsp;&emsp; $\\displaystyle \n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t)\n",
    "+ \\alpha \\Big(\n",
    "r_t + \\gamma \\max_a Q(s_{t+1}, a)\n",
    "- Q(s_t, a_t)\n",
    "\\Big)\n",
    "$\n",
    "\n",
    "&emsp;&emsp; $t \\leftarrow t + 1$  \n",
    "&emsp;&emsp; $s_t \\leftarrow s_{t+1}$  \n",
    "\n",
    "**yield** $Q$\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions\n",
    "\n",
    "The code skeleton for this problem is provided below. Implement the functions:\n",
    "\n",
    "- `double_q_learning`\n",
    "- `q_learning`\n",
    "\n",
    "After running both algorithms, you will obtain a plot of the estimated Q-value versus episode number.\n",
    "\n",
    "In your writeup:\n",
    "\n",
    "1. Compare the behavior of Q-Learning and Double Q-Learning.\n",
    "2. Explain why Q-Learning can exhibit maximization bias.\n",
    "3. Discuss the advantages and possible drawbacks of Double Q-Learning in general MDPs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from typing import Mapping, Iterator, TypeVar, Tuple, Dict, Iterable, Generic\n",
    "\n",
    "# RL imports (adapt or remove if you don't have the same environment):\n",
    "from rl.distribution import (\n",
    "    Distribution, Constant, Gaussian, Choose, SampledDistribution, Categorical\n",
    ")\n",
    "from rl.markov_process import NonTerminal, State, Terminal\n",
    "from rl.markov_decision_process import MarkovDecisionProcess\n",
    "from rl.td import epsilon_greedy_action\n",
    "\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# Tabular Q-value function approximation (done for you)\n",
    "# -----------------------------------------------------------------------\n",
    "class TabularQValueFunctionApprox(Generic[S, A]):\n",
    "    \"\"\"\n",
    "    A basic implementation of a tabular function approximation \n",
    "    with constant learning rate of 0.1\n",
    "    Also tracks the number of updates per (state, action).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.counts: Mapping[Tuple[NonTerminal[S], A], int] = defaultdict(int)\n",
    "        self.values: Mapping[Tuple[NonTerminal[S], A], float] = defaultdict(float)\n",
    "    \n",
    "    def update(self, k: Tuple[NonTerminal[S], A], target: float) -> None:\n",
    "        alpha = 0.1\n",
    "        old_val = self.values[k]\n",
    "        self.values[k] = (1 - alpha) * old_val + alpha * target\n",
    "        self.counts[k] += 1\n",
    "    \n",
    "    def __call__(self, x: Tuple[NonTerminal[S], A]) -> float:\n",
    "        return self.values[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# Double Q-Learning (fill in)\n",
    "# -----------------------------------------------------------------------\n",
    "def double_q_learning(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    states: Distribution[NonTerminal[S]],\n",
    "    gamma: float\n",
    ") -> Iterator[TabularQValueFunctionApprox[S, A]]:\n",
    "    \"\"\"\n",
    "    Implements Double Q-Learning as described:\n",
    "      1) We keep two Q-tables, Q1 and Q2.\n",
    "      2) We choose actions epsilon-greedily with respect to Q1+Q2.\n",
    "      3) With 50% chance we update Q1 using next-action chosen by max of Q2,\n",
    "         otherwise update Q2 using next-action chosen by max of Q1.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# Standard Q-Learning (fill in)\n",
    "# -----------------------------------------------------------------------\n",
    "def q_learning(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    states: Distribution[NonTerminal[S]],\n",
    "    gamma: float\n",
    ") -> Iterator[TabularQValueFunctionApprox[S, A]]:\n",
    "    \"\"\"\n",
    "    Standard Q-Learning:\n",
    "      1) Keep one Q table\n",
    "      2) Epsilon-greedy wrt that table\n",
    "      3) Update Q((s,a)) with  r + gamma * max_{a'} Q((s_next, a'))\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# The MDP: States A,B and actions a1,a2,b1,...,bn (don't modify anything anymore, just run to get the graphs)\n",
    "# -----------------------------------------------------------------------\n",
    "@dataclass(frozen=True)\n",
    "class P1State:\n",
    "    \"\"\"\n",
    "    The MDP state, storing whether we are in \"A\" or \"B\".\n",
    "    \"\"\"\n",
    "    name: str\n",
    "\n",
    "class P1MDP(MarkovDecisionProcess[P1State, str]):\n",
    "    \n",
    "    def __init__(self, n: int):\n",
    "        self.n = n\n",
    "\n",
    "    def actions(self, state: NonTerminal[P1State]) -> Iterable[str]:\n",
    "        \"\"\"\n",
    "        Return the actions available from this state.\n",
    "          - if state is A => [\"a1\", \"a2\"]\n",
    "          - if state is B => [\"b1\", ..., \"bn\"]\n",
    "        \"\"\"\n",
    "        if state.state.name == \"A\":\n",
    "            return [\"a1\", \"a2\"]\n",
    "        else:\n",
    "            return [f\"b{i}\" for i in range(1, self.n+1)]\n",
    "    \n",
    "    def step(\n",
    "        self,\n",
    "        state: NonTerminal[P1State],\n",
    "        action: str\n",
    "    ) -> Distribution[Tuple[State[P1State], float]]:\n",
    "        \"\"\"\n",
    "        Return the distribution of (next state, reward) from (state, action):\n",
    "          - A + a1 => reward 0, next state B\n",
    "          - A + a2 => reward 0, next state terminal\n",
    "          - B + b_i => reward ~ Normal(-0.1,1), next state terminal\n",
    "        \"\"\"\n",
    "        if state.state.name == \"A\":\n",
    "            if action == \"a1\":\n",
    "                return Constant((NonTerminal(P1State(\"B\")), 0.0))\n",
    "            else:\n",
    "                return Constant((Terminal(P1State(\"T\")), 0.0))\n",
    "        else:\n",
    "            # For B + b_i => reward ~ N(-0.1,1), then terminal\n",
    "            def sampler():\n",
    "                r = np.random.normal(loc=-0.1, scale=1.0)\n",
    "                return (Terminal(P1State(\"T\")), r)\n",
    "            return SampledDistribution(sampler)\n",
    "\n",
    "def run_double_q_once(mdp, start_dist, gamma=1.0, episodes=400):\n",
    "    \"\"\"\n",
    "    Runs one 'chain' of Double Q-Learning for 'episodes' episodes,\n",
    "    returning a list of Q-values for Q((A,a1)) at the end of each episode.\n",
    "    \"\"\"\n",
    "    dq_iter = double_q_learning(mdp, start_dist, gamma)  # generator\n",
    "    vals = []\n",
    "    for _ in range(episodes):\n",
    "        Q1 = next(dq_iter)\n",
    "        # record Q1((A,a1)) each time\n",
    "        qA1 = Q1((NonTerminal(P1State(\"A\")), \"a1\"))\n",
    "        vals.append(qA1)\n",
    "    return vals\n",
    "\n",
    "def run_q_once(mdp, start_dist, gamma=1.0, episodes=400):\n",
    "    \"\"\"\n",
    "    Same but for standard Q-Learning\n",
    "    \"\"\"\n",
    "    q_iter = q_learning(mdp, start_dist, gamma)\n",
    "    vals = []\n",
    "    for _ in range(episodes):\n",
    "        Q = next(q_iter)\n",
    "        qA1 = Q((NonTerminal(P1State(\"A\")), \"a1\"))\n",
    "        vals.append(qA1)\n",
    "    return vals\n",
    "\n",
    "def main():\n",
    "    # For reproducibility\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    n = 10\n",
    "    mdp = P1MDP(n)\n",
    "    # Always start in A, as a NonTerminal\n",
    "    start_dist = Constant(NonTerminal(P1State(\"A\")))\n",
    "\n",
    "    N_RUNS = 100\n",
    "    N_EPISODES = 400\n",
    "\n",
    "    all_dbl = []\n",
    "    all_std = []\n",
    "\n",
    "    for _ in range(N_RUNS):\n",
    "        dbl_vals = run_double_q_once(mdp, start_dist, gamma=1.0, episodes=N_EPISODES)\n",
    "        std_vals = run_q_once(mdp, start_dist, gamma=1.0, episodes=N_EPISODES)\n",
    "        all_dbl.append(dbl_vals)\n",
    "        all_std.append(std_vals)\n",
    "\n",
    "    arr_dbl = np.array(all_dbl)\n",
    "    arr_std = np.array(all_std)\n",
    "\n",
    "    avg_dbl = np.mean(arr_dbl, axis=0)\n",
    "    avg_std = np.mean(arr_std, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(avg_dbl, label='Double Q-Learning: Q(A,a1)')\n",
    "    plt.plot(avg_std, label='Q-Learning: Q(A,a1)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Estimated Q-value')\n",
    "    plt.title('Average Q(A,a1) over 100 runs, n=10')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec3b09674e90986d335e6a0f0769c6f4f6ef8e6ef66b01c16af34c6f4c8fd0e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
